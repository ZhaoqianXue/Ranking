{
  "timestamp": "2025-10-14T02:08:57.861473",
  "total_models": 52,
  "virtual_benchmarks": [
    "creative_writing_bt_prob",
    "math_bt_prob",
    "instruction_following_bt_prob",
    "coding_bt_prob",
    "hard_prompt_bt_prob",
    "longer_query_bt_prob",
    "multi_turn_bt_prob"
  ],
  "models": [
    "gemini-2.5-pro",
    "gemini-2.5-pro-preview-03",
    "grok-4-0709",
    "o3-2025-04-16",
    "chatgpt-4o-latest-2025032",
    "gemini-2.5-pro-preview-05",
    "deepseek-r1-0528",
    "grok-3-preview-02-24",
    "llama-4-maverick-03-26-ex",
    "gemini-2.5-flash",
    "qwen3-235b-a22b-no-thinki",
    "gemini-2.5-flash-preview-",
    "gemini-2.0-flash-thinking",
    "kimi-k2-0711-preview",
    "gpt-4.1-2025-04-14",
    "deepseek-v3-0324",
    "o4-mini-2025-04-16",
    "mistral-medium-2505",
    "gemini-2.5-flash-lite-pre",
    "qwen-max-2025-01-25",
    "hunyuan-turbos-20250416",
    "qwen3-235b-a22b",
    "minimax-m1",
    "claude-opus-4-20250514-th",
    "qwen3-235b-a22b-instruct-",
    "gemma-3-27b-it",
    "claude-opus-4-20250514",
    "grok-3-mini-beta",
    "grok-3-mini-high",
    "gpt-4o-2024-11-20",
    "claude-sonnet-4-20250514-",
    "gemini-2.0-flash-001",
    "mistral-small-2506",
    "gpt-4.1-mini-2025-04-14",
    "qwq-32b",
    "claude-sonnet-4-20250514",
    "qwen3-30b-a3b",
    "command-a-03-2025",
    "o3-mini",
    "amazon-nova-experimental-",
    "gemma-3n-e4b-it",
    "claude-3-7-sonnet-2025021",
    "claude-3-5-sonnet-2024102",
    "llama-4-maverick-17b-128e",
    "claude-3-7-sonnet-2025021",
    "llama-4-scout-17b-16e-ins",
    "llama-3.3-70b-instruct",
    "amazon.nova-pro-v1:0",
    "mistral-small-3.1-24b-ins",
    "magistral-medium-2506",
    "claude-3-5-haiku-20241022",
    "gpt-4o-mini-2024-07-18"
  ],
  "shape": [
    7,
    52
  ],
  "source": "Arena Human Preference Dataset via arena_data_collector.py",
  "prepared_for": "ranking_cli.R spectral ranking analysis",
  "min_games_threshold": 5,
  "category_min_games_threshold": 3,
  "virtual_benchmarks_explanation": {
    "win_rate": "Overall win rate (wins / total_games)",
    "effective_score": "Weighted score (win=1, tie=0.5, loss=0)",
    "total_games": "Total number of comparisons participated in",
    "tie_rate": "Tie rate (ties / total_games)",
    "creative_writing_bt_prob": "BT-MLE probability in creative writing tasks",
    "math_bt_prob": "BT-MLE probability in mathematics reasoning tasks",
    "instruction_following_bt_prob": "BT-MLE probability in instruction following tasks",
    "coding_bt_prob": "BT-MLE probability in programming/coding tasks",
    "hard_prompt_bt_prob": "BT-MLE probability in hard/complex prompt tasks",
    "longer_query_bt_prob": "BT-MLE probability in longer query tasks (>500 tokens)",
    "multi_turn_bt_prob": "BT-MLE probability in multi-turn conversation tasks"
  },
  "task_categories": {
    "Creative Writing": "Tasks requiring originality and imagination",
    "Math": "Mathematics and logical reasoning tasks",
    "Instruction Following": "Tasks requiring precise instruction execution",
    "Coding": "Programming and code-related tasks",
    "Hard Prompt": "Complex tasks meeting \u22656 difficulty criteria",
    "Longer Query": "Queries with >500 tokens",
    "Multi-Turn": "Conversations with >1 turns"
  }
}